<div class="preparsed-md">
<h1>A Probabilistic View on Linear Regression</h1>
<img src="blogposts/images/prob_view_on_lin_reg.jpeg" alt="Alt text for accessibility" width="500">
<p><em>Whiteboard musings from this morning</em></p>
<br>
<p><u> Why “least squares”? </u></p>
<p>This is by no means a new topic nor a particularly complex one, but I generally like to know where stuff in math or science really comes from.
If you’re reading this, you are probably familiar with Linear Regression or Least Squares Regression (or whatever other name you’ve heard it called).
The premise is that if we have some data points $y_i$ and we measure a series of features $\vec{x}^{(i)}$, if our features are relevant predictors of our data,
there ought to exist a linear (or affine) model which relates them:</p>
<div class="math-block">$$
\begin{align}
y_i = \theta_1 x^{(i)}_1 + \theta_2 x^{(i)}_2 + ... = \vec{\theta} \cdot \vec{x}^{(i)}
\end{align}
$$</div>
<p>We should think of $y_i$ as the output, and each entry ($n$ in total) of $\vec{x}^{(i)}$ as some feature. As good statisticians, we might measure $m$ such outputs $y_i$ and for each, their corresponding input feature vector $\vec{x}^{(i)}$. It is convenient to concatenate our $y_i$ into a column vector $\vec{y} \in \mathbb{R}^m$ and our $\vec{x}^{(i)}$ vectors as rows of a matrix $X \in \mathbb{R}^{m \times n}$. For every pair $(y_i, \vec{x}^{(i)})$ we will still use the same summation coefficients $\vec{\theta} \in \mathbb{R}^n$.</p>
<div class="math-block">$$
\begin{align}
\vec{y} = X \vec{\theta}
\end{align}
$$</div>
<p>I personally prefer working in index notation (the arguments scale easily to arbitrary dimensions) so I’ll stick to that. So far we have been dealing with this idealised scenario where there exists this <em>optimal</em> projection direction which we’ll denote as $\theta_j^{\ast}$. If this value exists, our optimisation task is simply to converge our model parameter $\theta_j$ to this value such that our model outputs $\hat{y}_i$ closely match the true measured values $y_i$. I want to validate this claim of there existing such an optimal parameter $\theta_j^{\ast}$.</p>
<br>
<p>Our measured data will almost surely not lie on a straight line, it’s noisy. We can think of each datum $y_i$ as being drawn from some distribution centred on mean value $\mu_i = X_{ij}\theta_j^{\ast}$ with <em>white noise</em> $\xi_i$. By white noise we mean a gaussian with $\mathbb{E}[\xi_i] = 0$ and $\mathbb{V}[\xi_i] = \sigma^2\delta_{ij}$ (zero-mean and spherical variance). Formally we have:</p>
<div class="math-block">$$
\begin{align}
Y_i \sim \mathcal{N}(\mu_i, \sigma^2 \delta_{ij})
\end{align}
$$</div>
<p>For each datum $y_i$ there is a corresponding distribution. All have the same variance but the mean will be different - importantly, the mean is always coupled to the input feature vector. Equivalently, we can think of the concatenated vector of measured outputs $\vec{y}$ as being a sample from:</p>
<div class="math-block">$$
\begin{align}
\vec{Y} \sim \mathcal{N}(\vec{\mu}, \sigma^2 \mathbb{I})
\end{align}
$$</div>
<p>Our goal parameter $\theta_j^{\ast}$ is inside the distribution’s mean. If we had many such vectors $\vec{y}_i$ all drawn from the same data matrix $X$, the job of finding the mean would be straightforward (we’ll get back to this later). What if we only have one such $\vec{y}$? Given an observed value of this random variable $Y_i$, we can assign a probability to this sample given a mean $\hat{\mu}_i$:</p>
<div class="math-block">$$
\begin{align}
\hat{\mu_i} &= X_{ij}\theta_j \\
\mathbb{P}[Y_i = y_i \mid X, \vec{\theta}] \propto \exp\Bigg[-\frac{1}{2} &(y_i - X_{ik}\theta_k) (\sigma^2 \delta_{ij})^{-1} (y_j - X_{jk}\theta_k) \Bigg]
\end{align}
$$</div>
<p>We of course want the maximum likelihood estimator (MLE) of this mean. For a string of $m$ such iid samples we can maximise the log-likelihood, omitting constant terms like the variance $\sigma^2$ or the normalisation factors, we have:</p>
<div class="math-block">$$
\begin{align}
\vec{\theta}^{\ast} &= \arg\max_{\theta} \left\{ \sum_{i=1}^{m} -\frac{1}{2} (y_i - X_{ik} \theta_k)^2 \right\} \\
&= \arg\min_{\theta} \left\{ \sum_{i=1}^{m} (y_i - X_{ik} \theta_k)^2 \right\}
\end{align}
$$</div>
<p>This is now the familiar form of the problem with solution $\vec{\theta} = (X^T X)^{-1}X^T \vec{y}$. Now we can revisit the suggestion of arriving at this answer by collecting lots of sample vectors $\vec{y}_i$ (all from the same data matrix $X$) and finding their arithemtic mean. The process would like like this:</p>
<div class="math-block">$$
\begin{align}
\langle \vec{y} \rangle &= \frac{1}{N} \sum_{i=1}^{N} \vec{y}_i \\
&= X\vec{\theta} \\
\rightarrow \vec{\theta} &= (X^T X)^{-1}X^T \langle \vec{y} \rangle
\end{align}
$$</div>
<p>Where we are implicitly using the fact that $\lim_{N \to \infty} \frac{1}{N} \sum_{i=1}^{N} \vec{y}_i = \mathbb{E}[\vec{y}]$ and the pseudo-inverse in the final line. Interestingly/unsurprisingly, we arrive at the same result.</p>
<br>
<p><u> Incorporating Priors: Regularisation </u></p>
<p>So far we have made no assumption about the distribution of our summation coefficients $\theta_j$. Suppose we are trying to predict height in a sample of people. We might collect feature data for each person including: parent height, person weight, parent weight, …, annual salary etc. Intuitively, some feature variables will be more important and require larger summation coefficient or <em>weighting</em> than others. More generally, we want to make use of any prior knowledge of the distribution of $\vec{\theta}$. This is known as regularisation. To do this, we should now maximise the log-likelihood of the joint probability $\mathbb{P}[y_i, \vec{\theta} \mid X] = \mathbb{P}[y_i \mid X, \vec{\theta}]\mathbb{P}[\vec{\theta}]$.</p>
<br>
<p>A common assumption is that the weights are also gaussian with $\theta_j \sim \mathcal{N}(0, \tau^2)$. Evaluating the product over $m$ iid samples gives a joint likelihood of:</p>
<div class="math-block">$$
\begin{align}
\mathbb{P}[\vec{\theta}] &\propto \prod_{j=1}^{n} \exp\Bigg[\frac{\theta_j^2}{2\tau^2} \Bigg] \\
\mathbb{P}[\vec{y}, \vec{\theta} \mid X] &\propto \prod_{i=1}^{m} \exp\Bigg[-\frac{1}{2\sigma^2} (y_i - X_{ik}\theta_k)^2 \Bigg] \prod_{j=1}^{n} \exp\Bigg[-\frac{\theta_j^2}{2\tau^2} \Bigg] \\
\vec{\theta}^{\ast} &= \arg\min_{\theta} \left\{ \sum_{i=1}^{m} \frac{1}{\sigma^2} (y_i - X_{ik} \theta_k)^2 + \sum_{j=1}^{n} \frac{\theta_j^2}{\tau^2} \right\} \\
&= \arg\min_{\theta} \left\{ \sum_{i=1}^{m} (y_i - X_{ik} \theta_k)^2 + \lambda \sum_{j=1}^{n} \theta_j^2 \right\} \\
\text{where } \lambda &= \frac{\sigma^2}{\tau^2}
\end{align}
$$</div>
<p>This is the <em>Ridge</em> regression which we interpret as punishing large coefficients (in a loss function sense). The analytic solution is now $\vec{\theta} = (X^T X + \lambda \mathbb{I})^{-1}X^T \vec{y}$. A full bias-variance analysis (a topic for another post) reveals that this estimator is (unsurprisingly) biased but with lower variance. By restricting our search for parameters which also satisfy our assumption of the underlying parameter distribution, the spread of our optimal parameter will intuitively be smaller at the cost of bias.</p>
<br>
<p>Another common assumption of the underlying distribution is the Laplace or double-exponential assumption. This corresponds to $\mathbb{P}[\theta_j] = \frac{1}{2b}\exp[-\frac{|\theta_j|}{b}]$. This symmetric distribution is much narrower than a gaussian with a much sharper gradient around zero. This shape is useful if we assume that not only are weights evenly distributed about zero, but many weights are <em>exactly</em> zero. Such an assumption is present in cases where we know that only a small subset of our feature space is relevant at all (but we don’t know apriori which features these are so we measure all of them anyway). An identical treatment to the above discussion on Ridge gives:</p>
<div class="math-block">$$
\begin{align}
\vec{\theta}^{\ast} &= \arg\min_{\theta} \left\{ \sum_{i=1}^{m}(y_i - X_{ik}\theta_k)^2 + \lambda \sum_{j=1}^{n} |\theta_j| \right\} \\
\text{where } \lambda &= \frac{2\sigma^2}{b}
\end{align}
$$</div>
<p>This is the <em>Lasso</em> regression which also punishes large coefficients but is less forgiving for very small ones (unlike Ridge). The absolute value function is not differentiable at zero so there is no analytic solution for $\vec{\theta}$. We usually proceed by sub-gradient descent (see <a href="https://davidrosenberg.github.io/mlcourse/Archive/2019/Lectures/03c.subgradient-descent-lasso.pdf">here</a>).</p>

</div>