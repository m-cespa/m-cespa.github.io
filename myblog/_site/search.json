[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This “blog” was largely motivated by friends who recommended I start writing the discussions I would otherwise burden them with over lunch - I reckon most blogs probably start like this. If you like science, philosophy or just interesting discussions I recommend Paul Graham’s website for some takes from someone far better than me.\nIf you find yourself here but do not have the time to read my posts I unfortunately am not relevant enough (yet) to warrant making a podcast. That said, I totally empathise with the sentiment so I’ve put my favourite two podcasts below to keep you entertained: Sean Carroll’s Mindscape, Quant Magazine’s Joy of Why.\nI recently got a whiteboard and find myself working on random problems, often over breakfast. I find it useful to write down what I work on - explaining stuff tends to help the explain-er just as much as the explain-ee. Some posts might be longer discussions on bits of maths/CS/physics I stumble across, others might be a one-off interesting problem. That’s the jist of this blog."
  },
  {
    "objectID": "posts/probabilistic_view_on_lin_reg.html",
    "href": "posts/probabilistic_view_on_lin_reg.html",
    "title": "A Probabilistic View on Linear Regression",
    "section": "",
    "text": "“Essentially, all models are wrong, but some are useful.” George E. P. Box\n\nIntroduction\nThis is by no means a new topic nor a particularly complex one, but I generally like to know where stuff in math or science really comes from. If you’re reading this, you are probably familiar with Linear Regression or Least Squares Regression (or whatever other name you’ve heard it called). The premise is that if we have some data points \\(y_i\\) and we measure a series of features \\(\\vec{x}^{(i)}\\), if our features are relevant predictors of our data, there ought to exist a linear (or affine) model which relates them:\n\\[\n\\begin{align}\ny_i = \\theta_1 x^{(i)}_1 + \\theta_2 x^{(i)}_2 + ... = \\vec{\\theta} \\cdot \\vec{x}^{(i)}\n\\end{align}\n\\]\nWe should think of \\(y_i\\) as the output, and each entry (\\(n\\) in total) of \\(\\vec{x}^{(i)}\\) as some feature. As good statisticians, we might measure \\(m\\) such outputs \\(y_i\\) and for each, their corresponding input feature vector \\(\\vec{x}^{(i)}\\). It is convenient to concatenate our \\(y_i\\) into a column vector \\(\\vec{y} \\in \\mathbb{R}^m\\) and our \\(\\vec{x}^{(i)}\\) vectors as rows of a matrix \\(X \\in \\mathbb{R}^{m \\times n}\\). For every pair \\((y_i, \\vec{x}^{(i)})\\) we will still use the same summation coefficients \\(\\vec{\\theta} \\in \\mathbb{R}^n\\).\n\\[\n\\begin{align}\n\\vec{y} = X \\vec{\\theta}\n\\end{align}\n\\]\nI personally prefer working in index notation (the arguments scale easily to arbitrary dimensions) so I’ll stick to that. So far we have been dealing with this idealised scenario where there exists this optimal projection direction which we’ll denote as \\(\\theta_j^{\\ast}\\). If this value exists, our optimisation task is simply to converge our model parameter \\(\\theta_j\\) to this value such that our model outputs \\(\\hat{y}_i\\) closely match the true measured values \\(y_i\\). I want to validate this claim of there existing such an optimal parameter \\(\\theta_j^{\\ast}\\).\n\n\n\nWhiteboard musings from this morning\n\n\n\n\nWhy Least Squares?\nWe can now start talking in terms of probability. Our measured data will almost surely not lie on a straight line, it’s noisy. We can think of each datum \\(y_i\\) as being drawn from some distribution centred on mean value \\(\\mu_i = X_{ij}\\theta_j^{\\ast}\\) with noise \\(\\xi_i\\). We take the noise to be gaussian with \\(\\mathbb{E}[\\xi_i] = 0\\) and \\(\\mathbb{V}[\\xi_i] = \\sigma^2\\delta_{ij}\\). Formally we have:\n\\[\n\\begin{align}\nY_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2 \\delta_{ij})\n\\end{align}\n\\]\nFor each datum \\(y_i\\) there is a corresponding distribution. All have the same variance but the mean will be different - importantly, the mean is always coupled to the input feature vector. Equivalently, we can think of the concatenated vector of measured outputs \\(\\vec{y}\\) as being a sample from:\n\\[\n\\begin{align}\n\\vec{Y} \\sim \\mathcal{N}(\\vec{\\mu}, \\sigma^2 \\mathbb{I})\n\\end{align}\n\\]\nOur goal parameter \\(\\theta_j^{\\ast}\\) is inside the distribution’s mean. If we had many such vectors \\(\\vec{y}_i\\) all drawn from the same data matrix \\(X\\), the job of finding the mean would be straightforward (we’ll get back to this later). What if we only have one such \\(\\vec{y}\\)? Given an observed value of this random variable \\(Y_i\\), we can assign a probability to this sample given a mean \\(\\hat{\\mu}_i\\):\n\\[\n\\begin{align}\n\\hat{\\mu_i} &= X_{ij}\\theta_j \\\\\n\\mathbb{P}[Y_i = y_i \\mid \\vec{\\theta}, X] \\propto \\exp\\Bigg[-\\frac{1}{2} &(y_i - X_{ik}\\theta_k) (\\sigma^2 \\delta_{ij})^{-1} (y_j - X_{jk}\\theta_k) \\Bigg]\n\\end{align}\n\\]\nWe of course want the maximum likelihood estimator (MLE) of this mean. For a string of \\(m\\) such iid samples we can maximise the log-likelihood, omitting constant terms like the variance \\(\\sigma^2\\) or the normalisation factors, we have:\n\\[\n\\begin{align}\n\\vec{\\theta}^{\\ast} &= \\arg\\max_{\\theta} \\left\\{ \\sum_{i=1}^{m} -\\frac{1}{2} (y_i - X_{ik} \\theta_k)^2 \\right\\} \\\\\n&= \\arg\\min_{\\theta} \\left\\{ \\sum_{i=1}^{m} (y_i - X_{ik} \\theta_k)^2 \\right\\}\n\\end{align}\n\\]\nThis is now the familiar form of the problem with solution \\(\\vec{\\theta} = (X^T X)^{-1}X^T \\vec{y}\\). Now we can revisit the suggestion of arriving at this answer by collecting lots of sample vectors \\(\\vec{y}_i\\) (all from the same data matrix \\(X\\)) and finding their arithemtic mean. The process would like like this:\n\\[\n\\begin{align}\n\\langle \\vec{y} \\rangle &= \\frac{1}{N} \\sum_{i=1}^{N} \\vec{y}_i \\\\\n&= X\\vec{\\theta} \\\\\n\\rightarrow \\vec{\\theta} &= (X^T X)^{-1}X^T \\langle \\vec{y} \\rangle\n\\end{align}\n\\]\nWhere we are implicitly using the fact that \\(\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{i=1}^{N} \\vec{y}_i = \\mathbb{E}[\\vec{y}]\\) and the pseudo-inverse in the final line. Interestingly/unsurprisingly, we arrive at the same result.\n\n\nIncorporating Priors\nSo far we have made no assumption about the distribution of our summation coefficients \\(\\theta_j\\). Suppose we are trying to predict height in a sample of people. We might collect feature data for each person including: parent height, person weight, parent weight, …, annual salary etc. Intuitively, some feature variables will be more important and require larger summation coefficient or weighting than others. More generally, we want to make use of any prior knowledge of the distribution of \\(\\vec{\\theta}\\). This is known as regularisation. To do this, we should now maximise the log-likelihood of the joint probability \\(\\mathbb{P}[y_i, \\vec{\\theta} \\mid X] = \\mathbb{P}[y_i \\mid \\vec{\\theta}, X]\\mathbb{P}[\\vec{\\theta} \\mid X]\\) (in this discussion I am ignoring any probabilistic features in \\(X\\) thus \\(\\mathbb{P}[\\vec{\\theta} \\mid X] \\equiv \\mathbb{P}[\\theta]\\), the conditioning is a bit of a notational overload)\nA common assumption is that the weights are also gaussian with \\(\\theta_j \\sim \\mathcal{N}(0, \\tau^2)\\). Evaluating the product over \\(m\\) iid samples gives a joint likelihood of:\n\\[\n\\begin{align}\n\\mathbb{P}[\\vec{\\theta}] &\\propto \\prod_{j=1}^{n} \\exp\\Bigg[\\frac{\\theta_j^2}{2\\tau^2} \\Bigg] \\\\\n\\mathbb{P}[\\vec{y}, \\vec{\\theta} \\mid X] &\\propto \\prod_{i=1}^{m} \\exp\\Bigg[-\\frac{1}{2\\sigma^2} (y_i - X_{ik}\\theta_k)^2 \\Bigg] \\prod_{j=1}^{n} \\exp\\Bigg[-\\frac{\\theta_j^2}{2\\tau^2} \\Bigg] \\\\\n\\vec{\\theta}^{\\ast} &= \\arg\\min_{\\theta} \\left\\{ \\sum_{i=1}^{m} \\frac{1}{\\sigma^2} (y_i - X_{ik} \\theta_k)^2 + \\sum_{j=1}^{n} \\frac{\\theta_j^2}{\\tau^2} \\right\\} \\\\\n&= \\arg\\min_{\\theta} \\left\\{ \\sum_{i=1}^{m} (y_i - X_{ik} \\theta_k)^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right\\} \\\\\n\\text{where } \\lambda &= \\frac{\\sigma^2}{\\tau^2}\n\\end{align}\n\\]\nThis is the Ridge regression which we interpret as punishing large coefficients (in a loss function sense). The analytic solution is now \\(\\vec{\\theta} = (X^T X + \\lambda \\mathbb{I})^{-1}X^T \\vec{y}\\). A full bias-variance analysis (a topic for another post) reveals that this estimator is (unsurprisingly) biased but with lower variance. By restricting our search for parameters which also satisfy our assumption of the underlying parameter distribution, the spread of our optimal parameter will intuitively be smaller at the cost of bias.\nAnother common assumption of the underlying distribution is the Laplace or double-exponential assumption. This corresponds to \\(\\mathbb{P}[\\theta_j] = \\frac{1}{2b}\\exp [-\\frac{|\\theta_j|}{b}]\\). This symmetric distribution is much narrower than a gaussian with a much sharper gradient around zero. This shape is useful if we assume that not only are weights evenly distributed about zero, but many weights are exactly zero. Such an assumption is present in cases where we know that only a small subset of our feature space is relevant at all (but we don’t know apriori which features these are so we measure all of them anyway). An identical treatment to the above discussion on Ridge gives:\n\\[\n\\begin{align}\n\\vec{\\theta}^{\\ast} &= \\arg\\min_{\\theta} \\left\\{ \\sum_{i=1}^{m}(y_i - X_{ik}\\theta_k)^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j| \\right\\} \\\\\n\\text{where } \\lambda &= \\frac{2\\sigma^2}{b}\n\\end{align}\n\\]\nThis is the Lasso regression which also punishes large coefficients but is less forgiving for very small ones (unlike Ridge). The absolute value function is not differentiable at zero so there is no analytic solution for \\(\\vec{\\theta}\\). We usually proceed by sub-gradient descent (see here)."
  },
  {
    "objectID": "posts/variances_on_sphere.html",
    "href": "posts/variances_on_sphere.html",
    "title": "Variances on a sphere",
    "section": "",
    "text": "“Entities are not to be multiplied unnecessarily” John Punch\n\nIntroduction\nThe following discussion is very much against the above citation of Ockham’s Razor but is nonetheless entertaining. The problem is a simple one and you probably know the solution too (you might even know multiple solutions - if that’s the case, this post is likely a waste of your time). What is the variance of any of the coordinate \\(x, y, z\\) on a unit sphere?\n\n\nGeometric Solution\nBefore doing anything fancy, it should be obvious without calculation that the expected value of any of the three cartesian coordinates is zero as the probability mass in the opposing hemispheres is equal. If you’re a physicist, the following is probably the solution you’ve seen before. We’ll focus on the \\(z\\) coordinate - by symmetry they all have the same variance. We could jump straight to a trig integral and get to the answer in a few lines, but that implies assuming quite a bit of knowledge. The approach is broadly going to be:\n\ntransform to spherical coordinates with \\(r=1\\)\ndetermine the joint pdf of being at some location \\((\\theta, \\phi)\\) on the sphere\nextract the marginal pdf of being at some polar angle \\(\\theta\\)\nevaluate the variance in \\(z\\) knowing that \\(z = \\cos \\theta\\)\n\nThe coordinate transform to spherical coordinates is:\n\\[\n\\begin{align}\nx &= r\\sin\\theta \\cos\\phi \\\\\ny &= r\\sin\\theta \\sin\\phi \\\\\nz &= r\\cos\\theta\n\\end{align}\n\\]\nTo get the joint pdf we can either do a bit of sketching, or work with Jacobians. I’ll cover both:\n\n\n\nAn attempt at a sketch of a sphere\n\n\nSo we have that our area element is \\(dS = \\sin\\theta d\\theta d\\phi\\). In much the same way that we would calculate the probability of landing at some radius \\(r\\) in a circle of radius \\(R\\) by evaluating the ratio of areas \\(\\frac{\\pi r^2}{\\pi R^2}\\), we will do the same here - our area now being \\(4\\pi\\):\n\\[\n\\begin{align}\n\\mathbb{P}[\\theta &lt; \\theta' &lt; \\theta + d\\theta, \\phi &lt; &\\phi' &lt; \\phi + d\\phi] = \\frac{1}{4\\pi}\\sin\\theta \\, d\\theta \\, d\\phi \\\\\np(\\theta, \\phi) &= \\frac{1}{4\\pi}\\sin\\theta \\\\\np(\\theta) = \\int_{\\phi=0}^{\\phi=2\\pi} &p(\\theta, \\phi) d\\phi = \\frac{1}{2} \\sin\\theta\n\\end{align}\n\\]\nWith the marginal, the solution is easy. Let’s revisit the Jacobian. For a coordinate transform from coordinates \\(x_i\\) to \\(x_j\\) we have that \\(J_{ij} = \\frac{\\partial x_i}{\\partial x_j}\\). The Jacobian is really the transformation operator between the two systems and thus its determinant will tell us how much the generalised volume changes between the two. If in coordinates \\(x_i\\) we have a volume \\(dx_i^{(1)} dx_i^{(2)} dx_i^{(3)}\\), in \\(x_j\\) we will have a volume \\(\\det(J) dx_j^{(1)} dx_j^{(2)} dx_j^{(3)}\\). For our purposes \\(x_i = (x, y, z)\\) and \\(x_j = (r, \\theta, \\phi)\\). I’ll spare you the dirty work, the determinant of our Jacobian turns out to be \\(r^2 \\sin\\theta\\) thus:\n\\[\n\\begin{align}\n\\iiint f(x, y, z)\\, dx\\, dy\\, dz = \\iiint f(r\\sin\\theta\\cos\\phi, r\\sin\\theta\\sin\\phi, r\\cos\\theta) r^2 \\sin\\theta \\, dr \\, d\\theta \\, d\\phi\n\\end{align}\n\\]\nOur radius is constant so we can drop that dimension and look at the double integral instead. Let’s substitute \\(f(x, y, z)\\) as the pdf on the surface of the sphere which is uniform with probability \\(\\frac{1}{4\\pi}\\):\n\\[\n\\begin{align}\n\\iint \\frac{1}{4\\pi} \\sin\\theta \\, d\\theta \\, d\\phi = 1 \\text{ (by normalisation of a pdf)}\n\\end{align}\n\\]\nWe are unsurprisingly back at the same result for the joint pdf \\(p(\\theta, \\phi)\\). To conclude the solution we perform our variance calculation:\n\\[\n\\begin{align}\n\\mathbb{V}[z] = \\mathbb{V}[\\cos\\theta] = \\int_{0}^{\\pi} p(\\theta) \\cos^2\\theta \\, d\\theta - \\Bigg[ \\int_{0}^{\\pi} p(\\theta) \\cos\\theta \\, d\\theta \\Bigg]^2 = \\frac{1}{3}\n\\end{align}\n\\]\n\n\nCDF Solution\nSuppose you have the marginal \\(p(\\theta) = \\frac{1}{2} \\sin\\theta\\), we could also perform a variable transform to recover the marginal \\(p(z)\\). If we define a random variable \\(Y = g(X)\\) on some interval where \\(g\\) is a deterministic monotonic function then:\n\\[\n\\begin{align}\n\\mathbb{P}[Y &lt; y] &= \\int_{-\\infty}^{g^{-1}(y)} p_X(x) \\, dx = \\mathbb{P}[X &lt; g^{-1}(y)] \\text{ for increasing $g$} \\\\\n&= \\int_{g^{-1}(y)}^{\\infty} p_X(x) \\, dx = \\mathbb{P}[X &gt; g^{-1}(y)] \\text{ for decreasing $g$}\n\\end{align}\n\\]\nIn our case the interval of interest is \\(\\theta \\in [0, \\pi]\\) with \\(z = g(\\theta) = \\cos\\theta\\) where \\(\\cos\\) is monotonically decreasing over this interval. So we have that \\(\\mathbb{P}[Z &lt; z] = \\frac{1}{2}(1 + z)\\) from which we get that \\(p_Z(z) = \\frac{1}{2}\\). To some, this uniform marginal might be intuitive. On quick inspection, it seems that closer to the poles, there infinitesimal rings of constant \\(z\\) have smaller area than near the equator. It turns out that the rate at which these infinitesimal areas shrink near the poles is balanced by the rate at which the \\(z\\) coordinate changes with \\(\\theta\\). Anyhow, from here we again get that \\(\\mathbb{V}[Z] = \\frac{1}{3}\\).\n\n\nSymmetry Solution\nThis is by far the most economical solution and relies only on linearity of expectations. Our constraint is that \\(x^2 + y^2 + z^2 = 1\\) thus:\n\\[\n\\begin{align}\n&\\mathbb{E}[X^2 + Y^2 + Z^2 \\mid X^2 + Y^2 + Z^2 = 1] = 1 \\\\\n3&\\mathbb{E}[Z^2 \\mid X^2 + Y^2 + Z^2 = 1] = 1 \\rightarrow  \\mathbb{V}[Z] = \\frac{1}{3} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "A Probabilistic View on Linear Regression\n\n\n\n\n\n\n\n\nNov 1, 2025\n\n\nMichele Cespa\n\n\n\n\n\n\n\n\n\n\n\n\nVariances on a sphere\n\n\n\n\n\n\n\n\nOct 15, 2025\n\n\nMichele Cespa\n\n\n\n\n\nNo matching items"
  }
]