<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Michele Cespa">
<meta name="dcterms.date" content="2025-11-19">

<title>What is Entropy? – myblog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-5b4ad623e5705c0698d39aec6f10cf02.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">myblog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/m-cespa"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">What is Entropy?</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Michele Cespa </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 19, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#boltzmann-entropy" id="toc-boltzmann-entropy" class="nav-link" data-scroll-target="#boltzmann-entropy">Boltzmann Entropy</a></li>
  <li><a href="#the-typical-set" id="toc-the-typical-set" class="nav-link" data-scroll-target="#the-typical-set">The Typical Set</a></li>
  <li><a href="#information-entropy" id="toc-information-entropy" class="nav-link" data-scroll-target="#information-entropy">Information Entropy</a></li>
  <li><a href="#other-information-theoretic-measures" id="toc-other-information-theoretic-measures" class="nav-link" data-scroll-target="#other-information-theoretic-measures">Other Information Theoretic Measures</a></li>
  <li><a href="#application-to-machine-learning" id="toc-application-to-machine-learning" class="nav-link" data-scroll-target="#application-to-machine-learning">Application to Machine Learning</a></li>
  <li><a href="#application-to-statistics" id="toc-application-to-statistics" class="nav-link" data-scroll-target="#application-to-statistics">Application to Statistics</a></li>
  <li><a href="#the-end" id="toc-the-end" class="nav-link" data-scroll-target="#the-end">The End</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p><em>“You should call it entropy, because nobody knows what entropy really is, so in a debate you will always have the advantage.” John von Neumann</em></p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>I come from a physics background and my favourite field is by far statistical physics or condensed matter. The tldr on why, is that the arguments that the physicsists of the past came up with are surprisingly generalisable considering they were devised to describe particles in a gas or electrons populating discrete energy levels. The quote above is (allegedly) from a discussion between John von Neumann and Claude Shannon on what to call the new measure he had constructed to talk about information.</p>
<p>Seeing as I have exams in a few weeks, this serves as a nice opportunity to summarise some things I have learned about entropy now from an information theoretic perspective.</p>
</section>
<section id="boltzmann-entropy" class="level1">
<h1>Boltzmann Entropy</h1>
<p>Firstly, we’ll derive the standard Boltzmann or configurational entropy formula which gives us a feel for what this object looks like. We’ll be using the Stirling approximation for factorials:</p>
<p><span class="math display">\[
\begin{align}
\ln N! \approx N \ln N - N
\end{align}
\]</span></p>
<p>The standard argument goes as follows. Suppose a system can only ever be in any one of <span class="math inline">\(M\)</span> energy states or <em>microstates</em>. Each microstate corresponds to a physical configuration of particles in the system. Certain microstates may be more probable than others. If we take <span class="math inline">\(N\)</span> observations of our system, we might find that the <span class="math inline">\(i^{th}\)</span> microstate occurs <span class="math inline">\(N_i\)</span> times. We can thus define some basic probabilistic quantities:</p>
<p><span class="math display">\[
\begin{align}
E &amp;= \frac{1}{N} \sum_{i=1}^{M} N_i E_i \\
N &amp;= \sum_{i=1}^{M} N_i
\end{align}
\]</span></p>
<p>This describes a system which can be in <span class="math inline">\(M\)</span> distinct microstates and we implicitly assume that <span class="math inline">\(N\)</span> is large enough that we sample over all possible microstates. The total energy is a constant here as we assume the system to be in equilibrium with a reservoir - we can’t have a closed system otherwise there can be no fluctuations. Denote the <span class="math inline">\(n^{th}\)</span> of N observations with the observed energy <span class="math inline">\(E^{(i)}_n\)</span>. Our sampling is represented by all strings <span class="math inline">\(E^{(i)}_1, E^{(i)}_2, E^{(j)}_3...\)</span> where the the <span class="math inline">\(i^{th}\)</span> energy appears <span class="math inline">\(N_i\)</span> times. The size of this set is:</p>
<p><span class="math display">\[
\begin{align}
\Omega(E) = \frac{N!}{\prod_{i=1}^{M} N_i !}
\end{align}
\]</span></p>
<p>Note that I write <span class="math inline">\(\Omega(E)\)</span> as we are under a constraint on the total system energy. We now turn this into an optimisation problem with the objective of solving for the microstate multiplicities which maximise the likelihood of our sample (everything is an MLE if you really think about it…). To do this we maximise this configurational size <span class="math inline">\(\Omega(E)\)</span> or as is often the case, its logarithm under constraint of total energy:</p>
<p><span class="math display">\[
\begin{align}
\ln \Omega_{\lambda} &amp;\approx N \ln N - N - \sum_{i=1}^{M} N_i \ln N_i + \sum_{i=1}^{M} N_i - \lambda (\frac{1}{N} \sum_{i=1}^{M} N_i E_i - E) \\
\frac{\partial \ln \Omega_{\lambda}}{\partial N_k} &amp;= -\ln N_k -N - \lambda \frac{E_k}{N} = 0 \\
\end{align}
\]</span></p>
<p>This gives a probability of the <span class="math inline">\(k^{th}\)</span> microstate <span class="math inline">\(p_k = \frac{1}{Z} \exp(-\beta E_k)\)</span> where the constant <span class="math inline">\(\beta\)</span> subsumes any relevant multiplicative factor on energy. By some more physical and thermodynamic arguments, we can conclude that <span class="math inline">\(\beta = \frac{1}{k_B T}\)</span> where <span class="math inline">\(k_B\)</span> is the boltzmann factor. I won’t go into detail on these arguments but there are many useful stack exchange threads on the topic such as <a href="https://physics.stackexchange.com/questions/660259/why-is-the-frac-1-beta-t-a-constant-boltzmann-constant">this one</a>.</p>
<p>This normalisation factor <span class="math inline">\(Z = \sum_{k=1}^{M} \exp(-\beta E_k)\)</span> is the canonical partition function which turns out to be enormously useful. That said, we still haven’t established what entropy is. This thing we call entropy is really just the log-likelihood we extremised to get here (up to a constant factor <span class="math inline">\(k_B\)</span> so everything has the right physical units).</p>
<p><span class="math display">\[
\begin{align}
S(E) &amp;= k_B \ln \Omega(E) \\
&amp;= k_B \Bigg[ \ln N! - \sum_{i=1}^{M} \ln N_i! \Bigg] \\
&amp;\approx k_B \sum_{i=1}^{M} N_i (\ln N - \ln N_i) \\
&amp;= -N k_B \sum_{i=1}^{M} \frac{N_i}{N} \ln \frac{N_i}{N} \\
&amp;= -N k_B \sum_{i=1}^{M} p_i \ln p_i
\end{align}
\]</span></p>
<p>The constant order factors are less interesting, the salient point here is that entropy is really the expected negative log-likelihood of a system with <span class="math inline">\(i\)</span> states each with probability <span class="math inline">\(p_i\)</span>. Now tying this back to thermodynamics, we know that systems with higher entropy are favoured by the <span class="math inline">\(2^{nd}\)</span> law which now physically motivates our otherwise purely probabilistic thought process for the optimisation problem.</p>
</section>
<section id="the-typical-set" class="level1">
<h1>The Typical Set</h1>
<p>Before tackling the information theoretic interpretation, I’m going to do things slightly in reverse to the standard order. The entropy is now, relatively clearly, a generalisable measure of expected negative log-likelihood. The object we extremised to get here was <span class="math inline">\(\Omega\)</span> representing the total number of distinct strings compatible with our sample of the system. We concluded that <span class="math inline">\(\Omega = \exp(\frac{S}{k_B})\)</span>.</p>
<p>The typical set now appears. The typical set <span class="math inline">\(\mathcal{A}\)</span> is the set of all strings of <span class="math inline">\(N\)</span> observations in the limit <span class="math inline">\(N \to \infty\)</span> which constitute almost all the probability mass. This is sometimes stated as the set of strings which are <em>almost surely</em> observed. We let <span class="math inline">\(X^N\)</span> denote the string <span class="math inline">\(X_1, X_2, ... X_N\)</span> of iid samples thus:</p>
<p><span class="math display">\[
\begin{align}
\lim_{N \to \infty} \sum_{i \in \mathcal{A}} p(X^N_i) = \lim_{N \to \infty} \sum_{i \in \mathcal{A}} \prod_{k=1}^{N} p(X_k^{(i)})= 1
\end{align}
\]</span></p>
<p>It is maybe obvious, maybe not, that all the elements in the typical set have equal probability so we can drop the index <span class="math inline">\(i\)</span> denoting the <span class="math inline">\(i^{th}\)</span> such element:</p>
<p><span class="math display">\[
\begin{align}
\prod_{k=1}^{N} p(X_k) &amp;= 1 / |\mathcal{A}| \\
\sum_{k=1}^{N} -\ln p(X_k) &amp;= \ln |\mathcal{A}|
\end{align}
\]</span></p>
<p>Using the weak law of large numbers we have that <span class="math inline">\(\frac{1}{N} \sum_{k=1}^{N} -\ln p(X_k)\)</span> converges in probability to the expected value of the negative log-likelihood <span class="math inline">\(\mathbb{E}_p[-\ln p(X)]\)</span>. Sticking with the strict boltzmann definition that <span class="math inline">\(S = -N k_B \sum_{i=1}^{M} p_i \ln p_i\)</span> and rearranging:</p>
<p><span class="math display">\[
\begin{align}
\ln |\mathcal{A}| &amp;= N \mathbb{E}_p[-\ln p(X)] \\
&amp;= S / k_B \\
|\mathcal{A}| &amp;= \exp(\frac{S}{k_B}) \\
\end{align}
\]</span></p>
<p>I think this is pretty neat. The optimal value of <span class="math inline">\(\Omega\)</span> after extremising with respect to the multiplicites <em>is</em> the size of the typical set. This makes intuitive sense retrospectively. I want to highlight that I’ve not been quite so rigorous in these last few steps after invoking the law of large numbers. Many of these equalities are only true in terms of convergence in probability - the <a href="https://en.wikipedia.org/wiki/Typical_set">wikipedia page</a> is quite good at covering this.</p>
</section>
<section id="information-entropy" class="level1">
<h1>Information Entropy</h1>
<p>Information theory presents some more pragmatic interpretations to some of these strange objects. The first is the notion of a surprisal or uncertainty function. If we have some distribution <span class="math inline">\(p(x)\)</span> for <span class="math inline">\(x \in \mathcal{X}\)</span> (some discrete alphabet of symbols <span class="math inline">\(\mathcal{X}\)</span>), the less likely a symbol is, the more we should be surprised if we observe it in a random sample from the distribution. We also want that a symbol with probability <span class="math inline">\(1\)</span> encodes no surprise at all and a symbol with probability <span class="math inline">\(0\)</span> encodes infinite surprise.</p>
<p>Conveniently, the function satisfying this is <span class="math inline">\(h = \ln \frac{1}{p(x)}\)</span> or <span class="math inline">\(-\ln p(x)\)</span>. The expected value of this surprisal is then our entropy which for reasons beyond my understanding is denoted <span class="math inline">\(H[X]\)</span> in IT. A graphic example helps here so let’s take the biased coin with <span class="math inline">\(p(x_1) = p\)</span> and <span class="math inline">\(p(x_2) = 1 - p\)</span> which has entropy <span class="math inline">\(-p \ln p - (1-p)\ln(1-p)\)</span>. This should be familiar to physicists as the entropy of a binary mixture.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/biased-coin.png" class="img-fluid figure-img" style="width:60.0%" alt="biased coin"></p>
<figcaption>Entropy and Surprisal of Biased Coin</figcaption>
</figure>
</div>
<p>The incredibly convenient property about logarithms is their additivity. This means we can now sum surprisals and entropies. If we observe two independent events with probabilities <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>, the surprisal is <span class="math inline">\(h(p) + h(q)\)</span>. Likewise for two independent random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> the total entropy is <span class="math inline">\(H[X] + H[Y]\)</span>. With this toolkit of entropy and typical set you can cover a remarkable amount of ground in coding theory. This post isn’t going to do that.</p>
<p>A final remark on this additivity property is how entropy lends a nice interpretation to conditional probability chain rules. Below are some examples:</p>
<p><span class="math display">\[
\begin{align}
p(X, Y) &amp;= p(X | Y)p(Y) \\
\text{ gives } H[X, Y] &amp;= H[X | Y] + H[Y] = H[Y | X] + H[X] \\
\\
p(X, Y | Z) &amp;= p(X | Y, Z)p(Y | Z) \\
\text{ gives } H[X, Y | Z] &amp;= H[X | Y, Z] + H[Y | Z]
\end{align}
\]</span></p>
<p>Where the entropies are defined as:</p>
<p><span class="math display">\[
\begin{align}
H[X, Y] &amp;= \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} -p(x, y) \ln p(x, y) \\
H[X | Y] &amp;= \sum_{y \in \mathcal{Y}} -p(y) \sum_{x \in \mathcal{X}} p(x | y) \ln p(x | y)
\end{align}
\]</span></p>
</section>
<section id="other-information-theoretic-measures" class="level1">
<h1>Other Information Theoretic Measures</h1>
<p>Having established the generality of entropy, we might ask what else we can do with it. Before that I want to present a diagram which I at first didn’t really get the point of at all but is actually a very good summary of the various forms of entropy when dealing with a bivariate distribution.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/entropy_venn.png" class="img-fluid figure-img" style="width:60.0%" alt="venn diagram of entropies"></p>
<figcaption>Entropy Venn Diagram</figcaption>
</figure>
</div>
<p>The way to read this diagram is:</p>
<ul>
<li><span class="math inline">\(H[X | Y]\)</span> is the left over uncertainty in <span class="math inline">\(X\)</span> if <span class="math inline">\(Y\)</span> is known - corresponds to <span class="math inline">\(X \cap Y^c\)</span> (reverse argument for <span class="math inline">\(H[Y | X]\)</span>)</li>
<li><span class="math inline">\(H[X, Y]\)</span> is the uncertainty in sampling both variables - corresponds to <span class="math inline">\(X \cup Y\)</span></li>
<li><span class="math inline">\(I(X ;Y)\)</span> is the amount of uncertainty which <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> share - corresponds to <span class="math inline">\(X \cap Y\)</span></li>
</ul>
<p>This motivates the notion of mutual information which is this <span class="math inline">\(I(X; Y)\)</span> (the ; represents symmetry in the arguments). We can actually assemble this object by inclusion-exclusion from the venn diagram:</p>
<p><span class="math display">\[
\begin{align}
I(X; Y) &amp;= H[X] + H[Y] - H[X, Y] \\
&amp;= H[X] - H[X | Y] \\
&amp;= H[Y] - H[Y | X]
\end{align}
\]</span></p>
<p>There are many interpretations of this object. The dominant one is that we can view a decrement in entropy as an increment in information (the converse of uncertainty is information). This is a useful view to take because with our understanding of entropy as a negative log-likelihood, many common optimisation problems commonly posed as maximising a product of likelihoods (maximise information) can be equivalently posed as minimising a sum of negative log-likelihoods or entropy (minimise uncertainty). The mutual information is then the information gained about <span class="math inline">\(X\)</span> after learning <span class="math inline">\(Y\)</span>.</p>
<p>A different interpretation comes from the first formulation which is concretely:</p>
<p><span class="math display">\[
\begin{align}
I(X; Y) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x, y) \ln \frac{p(x, y)}{p(x)p(y)}
\end{align}
\]</span></p>
<p>To interpret this I’ll briefly need to introduce the KL-Divergence which we can think of as a measure of distance between two distributions. We write it as <span class="math inline">\(D_{KL}(p || q)\)</span> and it satisfies <span class="math inline">\(D_{KL} \geq 0\)</span> (Gibb’s inequality). The formula for the bivariate case is:</p>
<p><span class="math display">\[
\begin{align}
D_{KL}(p(x, y) || q(x, y)) = \sum_{x \in \mathcal{X}}\sum_{y \in \mathcal{Y}} p(x, y) \ln \frac{p(x, y)}{q(x, y)}
\end{align}
\]</span></p>
<p>We generally view <span class="math inline">\(p\)</span> as the ground truth distribution and <span class="math inline">\(q\)</span> as some model distribution. The mutual information is then just <span class="math inline">\(D_{KL}(p(x, y) || p(x)p(y))\)</span> and represents the deviation from independence displayed in the joint distribution. The property that <span class="math inline">\(D_{KL} \geq 0\)</span> ensures that this mutual information cannot be negative. In other words, conditioning on something cannot decrease the information you know about your random variable of interest.</p>
</section>
<section id="application-to-machine-learning" class="level1">
<h1>Application to Machine Learning</h1>
<p>The big entropy related measure I’ve ommitted entirely here is the Cross-Entropy. I think to properly motivate the Cross-Entropy a study on logistic regression or source coding helps - another time. That said, the mutual information and KL-divergence come up all the time - unsurprisingly as most ML tasks boil down to maximising a sum of log-likelihoods.</p>
<p>Take a supervised learning scenario with dataset <span class="math inline">\(\mathcal{D} = \{x_i, y_i\}\)</span>. Our model will implicitly learn a distribution <span class="math inline">\(q(y | x, \theta)\)</span> (where <span class="math inline">\(\theta\)</span> is the set of model parameters). Letting the true underlying distribution be <span class="math inline">\(p(y | x)\)</span> we have <span class="math inline">\(D_{KL}(p || q) \geq 0\)</span> thus <span class="math inline">\(\mathbb{E}_p[\ln p] \geq \mathbb{E}_p[\ln q]\)</span>. Jumbling some algebra together we get:</p>
<p><span class="math display">\[
\begin{align}
I(X; Y) &amp;= H[Y] - H[Y | X] \\
&amp;= H[Y] + \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \ln p(x | y) \\
&amp;= H[Y] + \mathbb{E}_{p(x,y)}[ \ln p(y | x)] \\
&amp;\geq H[Y] + \mathbb{E}_{p(x,y)}[ \ln q(y | x, \theta)] \\
\end{align}
\]</span></p>
<p>We should hope that a good model has objective of maximising the mutual information between features and labels and we can see that this optimisation is bounded by the true value.</p>
</section>
<section id="application-to-statistics" class="level1">
<h1>Application to Statistics</h1>
<p>If you do a lot of hypothesis testing you will be familiar with the Neyman-Pearson likelihood ratio:</p>
<p><span class="math display">\[
\begin{align}
\Lambda = \frac{\sup_{\theta \in \Theta_1} p_{\theta}(x)}{\sup_{\theta \in \Theta_0} p_{\theta}(x)}
\end{align}
\]</span></p>
<p>(Note I am now using the frequentist notation for <span class="math inline">\(\theta\)</span> where it is not a random variable)</p>
<p>Likelihood and ratio already point to something like a KL-divergence. The rest is really just the same old trick. Take the case of hypothesis testing for dependence. Formally</p>
<p><span class="math display">\[
\begin{align}
\mathcal{H_0}: p_{\theta}(x, y) = f(x)g(y) \\
\mathcal{H_1}: p_{\theta}(x, y) \neq f(x)g(y) \\
\end{align}
\]</span></p>
<p>If we take <span class="math inline">\(N\)</span> iid samples from our distribution, we should look at the product of likelihood ratios…or its logarithm.</p>
<p><span class="math display">\[
\begin{align}
\ln \Lambda &amp;= \sum_{i=1}^{N} \ln p_{\theta_j}(x_i, y_i) - \ln p_{\theta_f}(x_i, y_i) \\
&amp;= N \mathbb{E}_{p(x, y)}[ \ln p_{\theta_j}(x_i, y_i)] - N \mathbb{E}_{p(x, y)} [\ln p_{\theta_f}(x_i, y_i) ] \\
&amp;\text{(in the weak law of large numbers limit)}
\end{align}
\]</span></p>
<p>(where I am writing the null hypothesis factorised distribution as <span class="math inline">\(p_{\theta_f}(x_i, y_i)\)</span>, the alternative hypothesis joint distribution as <span class="math inline">\(p_{\theta_j}(x_i, y_i)\)</span> and the true underlying distribution as <span class="math inline">\(p(x, y)\)</span>)</p>
<p>Looking at each term of the expression in turn we have for the first term:</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}_{p(x, y)}[ \ln p_{\theta_j}(x_i, y_i)] &amp;= \mathbb{E}_{p(x, y)}[ \ln p_{\theta_j}(x_i, y_i) \frac{p(x, y)}{p(x, y)}] \\
&amp;= -D_{KL}(p || p_{\theta_j}) - H[X, Y] \\
&amp;= -H[X, Y] \\
\end{align}
\]</span></p>
<p>(in the final line we take the extremal case where there does exist an optimal <span class="math inline">\(\theta_j\)</span> which makes the joint distribution from the alternative hypothesis exactly match the true underlying)</p>
<p>For the second term:</p>
<p><span class="math display">\[
\begin{align}
\mathbb{E}_{p(x, y)} [\ln p_{\theta_f}(x_i, y_i)] &amp;= -D_{KL}(p(x) || p_{\theta_f}(x)) - D_{KL}(p(y) || p_{\theta_f}(y)) - H[X] - H[Y] \\
&amp;= -H[X] - H[Y]
\end{align}
\]</span></p>
<p>(using a similar argument here for the extremal cases where the KL-divergence goes to zero)</p>
<p>Putting it together we get <span class="math inline">\(\ln \Lambda = N(H[X] + H[Y] - H[X, Y]) = N I(X; Y)\)</span>. This makes a lot of sense retrospectively. If our variables are in fact independent, we expect <span class="math inline">\(I = 0\)</span> which corresponds to a likelihood ratio of <span class="math inline">\(1\)</span> - insufficient evidence to reject the null. If our variables are dependent, we expect a large mutual information and likelihood ratio - reject the null. We also see that the log ratio is linear in <span class="math inline">\(N\)</span> thus more samples will always increase the likelihood ratio.</p>
</section>
<section id="the-end" class="level1">
<h1>The End</h1>
<p>One thing I want to make clear is that what I’ve presented is very much the configurational interpretation of entropy. The strange thing about entropy in physics, is that it appears in conjunction with energies and other thermodynamic quantities. This can of course be derived as well, I’m just not going to do it here. David Tong’s lecture notes do this very well - <a href="https://www.damtp.cam.ac.uk/user/tong/statphys/statphys.pdf?utm_source=chatgpt.com">here</a>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/m-cespa\.github\.io\/myblog\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>