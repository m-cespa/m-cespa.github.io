[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This “blog” was largely motivated by friends who recommended I start writing the discussions I would otherwise burden them with over lunch - I reckon most blogs probably start like this. If you like science, philosophy or just interesting discussions I recommend Paul Graham’s website for some takes from someone far better than me.\nIf you find yourself here but do not have the time to read my posts I unfortunately am not relevant enough (yet) to warrant making a podcast. That said, I totally empathise with the sentiment so I’ve put my favourite two podcasts below to keep you entertained: Sean Carroll’s Mindscape, Quant Magazine’s Joy of Why.\nI recently got a whiteboard and find myself working on random problems, often over breakfast. I find it useful to write down what I work on - explaining stuff tends to help the explain-er just as much as the explain-ee. Some posts might be longer discussions on bits of maths/CS/physics I stumble across, others might be a one-off interesting problem. That’s the jist of this blog."
  },
  {
    "objectID": "posts/infogeom.html",
    "href": "posts/infogeom.html",
    "title": "Some thoughts on Information Geometry",
    "section": "",
    "text": "“Classical thermodynamics … is the only physical theory of universal content which I am convinced … will never be overthrown.” Albert Einstein\n\nIntroduction\nVery much continuing from the theme of the last post, I want to talk about some pretty elegant approaches to statistical physics - motivated purely from the statistical point of view. I think much of the astonishing success of thermodynamics (as Einstein clearly agrees) can be attributed to how well it abstracts ideas away from the noisy physical properties of a system. To tackle these topics properly, I’ll need to introduce quite a few things from the statistical toolbox, so bear with me for a bit…\n\n\nProjections\nIn geometry we are often interested in “shortest distances” which motivates the notion of orthogonal projections. The standard Euclidean projection can be formulated in a few ways but the one useful to us will be:\nlet \\(Q = \\vec{l} \\cdot \\vec{x}\\) define a plane and \\(P\\) be some point we want to project onto the plane: \\[\n\\begin{align}\nproj(P, Q) = \\arg\\min_{x'} || Q(x') - P ||^2\n\\end{align}\n\\]\nThis is nothing new, but serves as a reminder that projection is an extremisation problem and often we deal with the convenient Euclidean distance, which among other things is symmetric in its arguments. The KL-Divergence will serve as our “distance” measure when talking about probability distributions. We can now think of the planes we project onto as being manifolds where the coordinates are now the parameters of the distribution - the manifold of gaussians would be parametrised by the coordinate tuple \\((\\mu, \\sigma)\\). Similar to the Euclidean projection we can define the following:\nlet \\(p \\in \\mathcal{P}\\) be our distribution of interest and \\(\\mathcal{Q}\\) be the manifold embedded in \\(\\mathcal{P}\\) onto which we want to project: \\[\n\\begin{align}\nproj_M(p, \\mathcal{Q}) &= \\arg\\min_{q \\in \\mathcal{Q}} D(p || q) \\\\\nproj_I(p, \\mathcal{Q}) &= \\arg\\min_{q \\in \\mathcal{Q}} D(q || p)\n\\end{align}\n\\]\nThe KL-Divergence (\\(D(p || q)\\)) is in general not symmetric in its arguments and hence is not quite a “distance” measure - although locally is is and defines the Fisher Information metric. This asymmetry introduces two such projections which for now I’ll denote “M” and “I”.\n\n\nExponential Families\nAnother ingredient we’ll need is the notion of an exponential family. These functional forms turn out to be a very convenient way to express distributions. An exponential family for points \\(x \\in \\mathbb{R}^n\\) is characterised by: - an input subspace \\(S \\subseteq \\mathbb{R^n}\\) - a base measure \\(h: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\) - a set of features \\(\\mathbf{T}(x) = (T_1(x),..., T_m(x))\\) with \\(T_i: \\mathbb{R}^n \\rightarrow \\mathbb{R}\\)\nThe exponential family is then:\n\\[\n\\begin{align}\np_{\\theta}(x) &= \\frac{1}{Z}\\exp[\\mathbf{\\theta} \\cdot \\mathbf{T}(x)]h(x) \\\\\nZ &= \\sum_{x \\in S} \\exp[\\mathbf{\\theta} \\cdot \\mathbf{T}(x)]h(x)\n\\end{align}\n\\]\n\\(Z\\) is a partition function which turns out to be an immensely valuable object. For convenience, this \\(Z\\) is sometimes subsumed into the exponent and written as \\(G(\\theta) = \\log Z\\) - the log partition function. Most common distributions we deal with (Bernoulli, Beta, Poisson, Gaussian etc) can be written in this form but the parameters \\(\\theta\\) may not turn out to be the ones we are familiar with. I’ll demonstrate for the case of a 1D Gaussian:\nIn this case: \\(S = \\mathbb{R}\\), \\(h = \\frac{1}{\\sqrt{2\\pi}}\\), \\(\\mathbf{T}(x) = (x, x^2)\\) \\[\n\\begin{align}\np_{\\theta}(x) = \\frac{1}{\\sqrt{2\\pi}}\\exp[(x, x^2) \\cdot (\\frac{\\mu}{\\sigma^2}, \\frac{-1}{2\\sigma^2}) - (\\frac{\\mu}{2\\sigma^2} + \\log\\sigma)]\n\\end{align}\n\\]\nSo the natural parameters are not the familiar \\((\\mu, \\sigma)\\) pair but instead \\((\\frac{\\mu}{\\sigma^2}, \\frac{-1}{2\\sigma^2})\\) and the log partition function is \\(G(\\theta) = \\frac{\\mu}{2\\sigma^2} + \\log\\sigma\\).\n\n\nM-Projection and MLE\nTo convince you there is in fact a point to all this, let’s consider the gradient of \\(G\\):\n\\[\n\\begin{align}\nG(\\theta) &= \\log Z = \\log\\Bigg[\\sum_{x \\in S} \\exp[\\mathbf{\\theta} \\cdot \\mathbf{T}(x)] h(x) \\Bigg] \\\\\n\\frac{\\partial G}{\\partial \\theta_k} &= \\frac{1}{Z} \\sum_{x \\in S} T_k(x) \\exp[\\mathbf{\\theta} \\cdot \\mathbf{T}(x)] h(x) = \\mathbb{E}[T_k(x)]\n\\end{align}\n\\]\nThis is interesting. The gradient of the log partition function is exactly the expected value of our feature vector. What if we use an exponential family as our parametric distribution when performing an MLE?\n\\[\n\\begin{align}\n\\mathcal{L} = \\sum_{i=1}^N \\log p_{\\theta}(x_i) &= \\sum_{i=1}^N \\mathbf{\\theta} \\cdot \\mathbf{T}(x_i) - G(\\theta) + \\log h(x_i) \\\\\n&= \\theta \\cdot \\sum_{i=1}^N \\mathbf{T}(x_i) - NG(\\theta) + \\sum_{i=1}^N \\log h(x_i) \\\\\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_k} &= \\sum_{i=1}^N T_k(x_i) - N\\frac{\\partial G}{\\partial  \\theta_k} = 0 \\\\\n\\frac{\\partial G}{\\partial \\theta_k} &= \\frac{1}{N}\\sum_{i=1}^N T_k(x_i) = \\mathbb{E}[T_k(x)]\n\\end{align}\n\\]\nThis is even better. The MLE distribution is such that the sample averages of the features correspond exactly to the expected values. Tying this back to projections, we can consider the likelihood maximisation in terms of KL-Divergence:\nFor notational convenience, let \\(p\\) be the ground truth distribution and our model (previously \\(p_{\\theta}\\)) be \\(q \\in \\mathcal{Q}\\) where \\(\\mathcal{Q}\\) is the manifold parametrised by \\(\\theta\\).\n\\[\n\\begin{align}\n\\max_{q \\in \\mathcal{Q}} \\mathcal{L} &= \\max_{q \\in \\mathcal{Q}} \\frac{1}{N} \\sum_{i=1}^N \\log q(x_i) \\\\\n&\\approx \\max_{q \\in \\mathcal{Q}} \\mathbb{E}_p[\\log q] \\\\\n&= \\min_{q \\in \\mathcal{Q}} -\\mathbb{E}_p[\\log q] \\\\\n&= \\min_{q \\in \\mathcal{Q}} D(p || q) \\\\\n\\end{align}\n\\]\nTherefore the MLE distribution is the M-projection of the ground truth \\(p\\) onto the manifold of our parametrised model \\(\\mathcal{Q}\\). If we choose for this manifold to describe an exponential family, then the expected values of our MLE distribution will match exactly the sample averages.\n\n\nI-Projection and MaxEnt\nSometimes we actually don’t have any individual data points at all, but we may have some moments which we wish to constrain our model to preserving. We’ll get to the thermodynamics later, but take a classic thermal reservoir where we can talk about internal energy \\(U\\) which is a macroscopic average, the data points would consitute individual samples of energy which (suppose for example fluctuations are extremely fast) may be beyond the resolution of our measuring instrument (a simple thermometer). Anyhow, what to do?\nIf we only have moments as prior knowledge, and we know that unfortunately the moments of a distribution do not uniquely characterise a distribution, it might be difficult to choose a parametric model. So we don’t, we instead produce the most unbiased distribution we can, the maximum entropy (MaxEnt) distribution - subject to these constraints.\nLet \\(\\alpha_i = \\mathbb{E}[T_i(x)] = \\sum_{x \\in \\mathcal{X}} q(x) T_i(x)\\) be the fixed values for our moments.\n\\[\n\\begin{align}\nH_{\\theta} &= -\\sum_{x \\in \\mathcal{X}} q(x) \\log q(x) + \\sum_i \\theta_i(\\alpha_i - \\sum_{x \\in \\mathcal{X}} q(x) T_i(x)) + \\beta(1 - \\sum_{x \\in \\mathcal{X}} q(x))\\\\\nq_{\\theta} &= \\max_{q} H_{\\theta} \\\\\n&= \\min_{q} -H_{\\theta} + H(q, u) \\text{ (u is the uniform distribution)} \\\\\n&= \\min_{q} D_{\\theta}(q || u) \\\\\n&= \\frac{1}{Z}\\exp[-\\sum_j \\theta_j T_j(x)]\n\\end{align}\n\\]\nThis is an I-projection. We are looking for the distribution \\(q\\) which is closest to the uniform distribution (under our constraints). The convenient thing about this formalism is the ease of introducing a non-uniform prior. If we have a non-uniform prior we replace the \\(u\\) in the KL-Divergence with our prior \\(p\\). Before actually doing this, let’s just convince ourselves this is in fact valid:\n\\[\n\\begin{align}\n-D(q || p) = H(q) - H(q, p)\n\\end{align}\n\\]\nWe want to maximise the entropy in \\(q\\) and minimise the cross-entropy between \\(q\\) and our prior \\(p\\) (\\(H(q, p)\\) is minimal at \\(q = p\\) by Gibb’s Inequality). This does in fact correspond to maximising \\(-D(q || p)\\) or equivalently minimising \\(D(q || p)\\). The result of this is:\n\\[\n\\begin{align}\nq_{\\theta} &= \\min_{q} D_{\\theta}(q || p) \\\\\n&= \\frac{1}{Z}\\exp[-\\mathbf{\\theta} \\cdot \\mathbf{T}(x)] p(x)\n\\end{align}\n\\]\nThis is an exponential family where the base measure \\(h(x)\\) is our prior distribution and the features are our imposed moments.\n\n\nWhat does Bayes think?\nI’ve now used the word prior enough times to warrant a quick detour into the bayesian perspective on things. My treatment thus far has been purely frequentist - any prior knowledge has been encoded as a “target” distribution we minimise distance to not as a bayesian prior. This isn’t a discussion on bayesian inference so I’ll keep the example brief.\nConsider the standard exercise of estimating the MAP parameter(s) \\(\\hat{\\theta}_{MAP}\\) and the associated distribution in the limit of a large dataset:\n\\[\n\\begin{align}\n\\hat{\\theta}_{MAP} &= \\arg\\max_{\\theta} \\lim_{N \\rightarrow \\infty} \\sum_{i=1}^N \\log p(x_i | \\theta) + \\log p(\\theta) \\\\\n&= \\arg\\max_{\\theta} \\lim_{N \\rightarrow \\infty} \\sum_{i=1}^N \\log p(x_i | \\theta) \\\\\n&= \\hat{\\theta}_{MLE} \\\\\n\\end{align}\n\\]\nBut we know that the MLE distribution (also in the limit of large \\(N\\)) corresponds to an M-projection from the ground truth onto the manifold of our parametrisation. So in the limit of large data, the MAP estimator converges to the MLE (nothing new) which in turn corresponds to a projection (new bit).\n\n\nThermodynamics\nLet’s derive the canonical partition function through the MaxEnt principle. We have a thermal system with internal energy \\(U\\) (an average over the energy all microstates). We are interested in the probability of the system being in the \\(i^{th}\\) microstate, we call this \\(q_i\\).\n\\[\n\\begin{align}\nH_{\\beta} &= -\\sum_i q_i \\log q_i + \\beta(U - \\sum_i q_i E_i) + \\gamma(1 - \\sum_i q_i) \\\\\nq_i &= \\arg\\max_{q} H_{\\beta} = \\frac{1}{Z} e^{-\\beta E_i} \\\\\nZ & = \\sum_i e^{-\\beta E_i}\n\\end{align}\n\\]\nOf course this \\(\\beta\\) parameter is the familiar \\(1/k_BT\\). Annoyingly for the pure statisticians, to obtain this relation we do need to actually invoke some physics, albeit not much.\nThe first law of thermodynamics states that: \\(dU = TdS - pdV\\) from which we derive that \\(\\frac{1}{T} = (\\frac{\\partial S}{\\partial U})_V\\). I derived the physical entropy \\(S\\) in my previous blog post.\n\\[\n\\begin{align}\nS &= -k_B \\sum_i q_i \\log q_i \\\\\n&= -k_B \\sum_i q_i (-\\log Z - \\beta E_i) \\\\\n&= k_B \\log Z + \\beta k_B U \\\\\n\\frac{\\partial S}{\\partial U} &= \\beta k_B = 1/T \\\\\n\\beta &= 1/k_BT\n\\end{align}\n\\]\nWhat we did here was an I-projection with uniform prior while fixing the expected value of energy to \\(U\\). As we noted earlier, the result is a member of an exponential family which means that the gradient of the log partition function recovers our energy \\(U\\) - a common result in thermodynamics.\n\n\nThe End\nI’ve necessarily been brief and less rigorous in places because I think what’s most important to take from study of these things is the intuition of what the framework is telling you, the maths can sometimes get in the way. For more rigorous treatments I recommend any of the following:\n\nMicrosoft Research Lecture\nEdwin Jayne’s Seminal Paper\nMaxEnt Wikipedia Page"
  },
  {
    "objectID": "posts/entropy.html",
    "href": "posts/entropy.html",
    "title": "What is Entropy?",
    "section": "",
    "text": "“You should call it entropy, because nobody knows what entropy really is, so in a debate you will always have the advantage.” John von Neumann\n\nIntroduction\nI come from a physics background and my favourite field is by far statistical physics or condensed matter. The tldr on why, is that the arguments that the physicsists of the past came up with are surprisingly generalisable considering they were devised to describe particles in a gas or electrons populating discrete energy levels. The quote above is (allegedly) from a discussion between John von Neumann and Claude Shannon on what to call the new measure he had constructed to talk about information.\nSeeing as I have exams in a few weeks, this serves as a nice opportunity to summarise some things I have learned about entropy now from an information theoretic perspective.\n\n\nBoltzmann Entropy\nFirstly, we’ll derive the standard Boltzmann or configurational entropy formula which gives us a feel for what this object looks like. We’ll be using the Stirling approximation for factorials:\n\\[\n\\begin{align}\n\\ln N! \\approx N \\ln N - N\n\\end{align}\n\\]\nThe standard argument goes as follows. Suppose a system can only ever be in any one of \\(M\\) energy states or microstates. Each microstate corresponds to a physical configuration of particles in the system. Certain microstates may be more probable than others. If we take \\(N\\) observations of our system, we might find that the \\(i^{th}\\) microstate occurs \\(N_i\\) times. We can thus define some basic probabilistic quantities:\n\\[\n\\begin{align}\nE &= \\frac{1}{N} \\sum_{i=1}^{M} N_i E_i \\\\\nN &= \\sum_{i=1}^{M} N_i\n\\end{align}\n\\]\nThis describes a system which can be in \\(M\\) distinct microstates and we implicitly assume that \\(N\\) is large enough that we sample over all possible microstates. The total energy is a constant here as we assume the system to be in equilibrium with a reservoir - we can’t have a closed system otherwise there can be no fluctuations. Denote the \\(n^{th}\\) of N observations with the observed energy \\(E^{(i)}_n\\). Our sampling is represented by all strings \\(E^{(i)}_1, E^{(i)}_2, E^{(j)}_3...\\) where the the \\(i^{th}\\) energy appears \\(N_i\\) times. The size of this set is:\n\\[\n\\begin{align}\n\\Omega(E) = \\frac{N!}{\\prod_{i=1}^{M} N_i !}\n\\end{align}\n\\]\nNote that I write \\(\\Omega(E)\\) as we are under a constraint on the total system energy. We now turn this into an optimisation problem with the objective of solving for the microstate multiplicities which maximise the likelihood of our sample (everything is an MLE if you really think about it…). To do this we maximise this configurational size \\(\\Omega(E)\\) or as is often the case, its logarithm under constraint of total energy:\n\\[\n\\begin{align}\n\\ln \\Omega_{\\lambda} &\\approx N \\ln N - N - \\sum_{i=1}^{M} N_i \\ln N_i + \\sum_{i=1}^{M} N_i - \\lambda (\\frac{1}{N} \\sum_{i=1}^{M} N_i E_i - E) \\\\\n\\frac{\\partial \\ln \\Omega_{\\lambda}}{\\partial N_k} &= -\\ln N_k -N - \\lambda \\frac{E_k}{N} = 0 \\\\\n\\end{align}\n\\]\nThis gives a probability of the \\(k^{th}\\) microstate \\(p_k = \\frac{1}{Z} \\exp(-\\beta E_k)\\) where the constant \\(\\beta\\) subsumes any relevant multiplicative factor on energy. By some more physical and thermodynamic arguments, we can conclude that \\(\\beta = \\frac{1}{k_B T}\\) where \\(k_B\\) is the boltzmann factor. I won’t go into detail on these arguments but there are many useful stack exchange threads on the topic such as this one.\nThis normalisation factor \\(Z = \\sum_{k=1}^{M} \\exp(-\\beta E_k)\\) is the canonical partition function which turns out to be enormously useful. That said, we still haven’t established what entropy is. This thing we call entropy is really just the log-likelihood we extremised to get here (up to a constant factor \\(k_B\\) so everything has the right physical units).\n\\[\n\\begin{align}\nS(E) &= k_B \\ln \\Omega(E) \\\\\n&= k_B \\Bigg[ \\ln N! - \\sum_{i=1}^{M} \\ln N_i! \\Bigg] \\\\\n&\\approx k_B \\sum_{i=1}^{M} N_i (\\ln N - \\ln N_i) \\\\\n&= -N k_B \\sum_{i=1}^{M} \\frac{N_i}{N} \\ln \\frac{N_i}{N} \\\\\n&= -N k_B \\sum_{i=1}^{M} p_i \\ln p_i\n\\end{align}\n\\]\nThe constant order factors are less interesting, the salient point here is that entropy is really the expected negative log-likelihood of a system with \\(i\\) states each with probability \\(p_i\\). Now tying this back to thermodynamics, we know that systems with higher entropy are favoured by the \\(2^{nd}\\) law which now physically motivates our otherwise purely probabilistic thought process for the optimisation problem.\n\n\nThe Typical Set\nBefore tackling the information theoretic interpretation, I’m going to do things slightly in reverse to the standard order. The entropy is now, relatively clearly, a generalisable measure of expected negative log-likelihood. The object we extremised to get here was \\(\\Omega\\) representing the total number of distinct strings compatible with our sample of the system. We concluded that \\(\\Omega = \\exp(\\frac{S}{k_B})\\).\nThe typical set now appears. The typical set \\(\\mathcal{A}\\) is the set of all strings of \\(N\\) observations in the limit \\(N \\to \\infty\\) which constitute almost all the probability mass. This is sometimes stated as the set of strings which are almost surely observed. We let \\(X^N\\) denote the string \\(X_1, X_2, ... X_N\\) of iid samples thus:\n\\[\n\\begin{align}\n\\lim_{N \\to \\infty} \\sum_{i \\in \\mathcal{A}} p(X^N_i) = \\lim_{N \\to \\infty} \\sum_{i \\in \\mathcal{A}} \\prod_{k=1}^{N} p(X_k^{(i)})= 1\n\\end{align}\n\\]\nIt is maybe obvious, maybe not, that all the elements in the typical set have equal probability so we can drop the index \\(i\\) denoting the \\(i^{th}\\) such element:\n\\[\n\\begin{align}\n\\prod_{k=1}^{N} p(X_k) &= 1 / |\\mathcal{A}| \\\\\n\\sum_{k=1}^{N} -\\ln p(X_k) &= \\ln |\\mathcal{A}|\n\\end{align}\n\\]\nUsing the weak law of large numbers we have that \\(\\frac{1}{N} \\sum_{k=1}^{N} -\\ln p(X_k)\\) converges in probability to the expected value of the negative log-likelihood \\(\\mathbb{E}_p[-\\ln p(X)]\\). Sticking with the strict boltzmann definition that \\(S = -N k_B \\sum_{i=1}^{M} p_i \\ln p_i\\) and rearranging:\n\\[\n\\begin{align}\n\\ln |\\mathcal{A}| &= N \\mathbb{E}_p[-\\ln p(X)] \\\\\n&= S / k_B \\\\\n|\\mathcal{A}| &= \\exp(\\frac{S}{k_B}) \\\\\n\\end{align}\n\\]\nI think this is pretty neat. The optimal value of \\(\\Omega\\) after extremising with respect to the multiplicites is the size of the typical set. This makes intuitive sense retrospectively. I want to highlight that I’ve not been quite so rigorous in these last few steps after invoking the law of large numbers. Many of these equalities are only true in terms of convergence in probability - the wikipedia page is quite good at covering this.\n\n\nInformation Entropy\nInformation theory presents some more pragmatic interpretations to some of these strange objects. The first is the notion of a surprisal or uncertainty function. If we have some distribution \\(p(x)\\) for \\(x \\in \\mathcal{X}\\) (some discrete alphabet of symbols \\(\\mathcal{X}\\)), the less likely a symbol is, the more we should be surprised if we observe it in a random sample from the distribution. We also want that a symbol with probability \\(1\\) encodes no surprise at all and a symbol with probability \\(0\\) encodes infinite surprise.\nConveniently, the function satisfying this is \\(h = \\ln \\frac{1}{p(x)}\\) or \\(-\\ln p(x)\\). The expected value of this surprisal is then our entropy which for reasons beyond my understanding is denoted \\(H[X]\\) in IT. A graphic example helps here so let’s take the biased coin with \\(p(x_1) = p\\) and \\(p(x_2) = 1 - p\\) which has entropy \\(-p \\ln p - (1-p)\\ln(1-p)\\). This should be familiar to physicists as the entropy of a binary mixture.\n\n\n\nEntropy and Surprisal of Biased Coin\n\n\nThe incredibly convenient property about logarithms is their additivity. This means we can now sum surprisals and entropies. If we observe two independent events with probabilities \\(p\\) and \\(q\\), the surprisal is \\(h(p) + h(q)\\). Likewise for two independent random variables \\(X\\) and \\(Y\\) the total entropy is \\(H[X] + H[Y]\\). With this toolkit of entropy and typical set you can cover a remarkable amount of ground in coding theory. This post isn’t going to do that.\nA final remark on this additivity property is how entropy lends a nice interpretation to conditional probability chain rules. Below are some examples:\n\\[\n\\begin{align}\np(X, Y) &= p(X | Y)p(Y) \\\\\n\\text{ gives } H[X, Y] &= H[X | Y] + H[Y] = H[Y | X] + H[X] \\\\\n\\\\\np(X, Y | Z) &= p(X | Y, Z)p(Y | Z) \\\\\n\\text{ gives } H[X, Y | Z] &= H[X | Y, Z] + H[Y | Z]\n\\end{align}\n\\]\nWhere the entropies are defined as:\n\\[\n\\begin{align}\nH[X, Y] &= \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} -p(x, y) \\ln p(x, y) \\\\\nH[X | Y] &= \\sum_{y \\in \\mathcal{Y}} -p(y) \\sum_{x \\in \\mathcal{X}} p(x | y) \\ln p(x | y)\n\\end{align}\n\\]\n\n\nOther Information Theoretic Measures\nHaving established the generality of entropy, we might ask what else we can do with it. Before that I want to present a diagram which I at first didn’t really get the point of at all but is actually a very good summary of the various forms of entropy when dealing with a bivariate distribution.\n\n\n\nEntropy Venn Diagram\n\n\nThe way to read this diagram is:\n\n\\(H[X | Y]\\) is the left over uncertainty in \\(X\\) if \\(Y\\) is known - corresponds to \\(X \\cap Y^c\\) (reverse argument for \\(H[Y | X]\\))\n\\(H[X, Y]\\) is the uncertainty in sampling both variables - corresponds to \\(X \\cup Y\\)\n\\(I(X ;Y)\\) is the amount of uncertainty which \\(X\\) and \\(Y\\) share - corresponds to \\(X \\cap Y\\)\n\nThis motivates the notion of mutual information which is this \\(I(X; Y)\\) (the ; represents symmetry in the arguments). We can actually assemble this object by inclusion-exclusion from the venn diagram:\n\\[\n\\begin{align}\nI(X; Y) &= H[X] + H[Y] - H[X, Y] \\\\\n&= H[X] - H[X | Y] \\\\\n&= H[Y] - H[Y | X]\n\\end{align}\n\\]\nThere are many interpretations of this object. The dominant one is that we can view a decrement in entropy as an increment in information (the converse of uncertainty is information). This is a useful view to take because with our understanding of entropy as a negative log-likelihood, many common optimisation problems commonly posed as maximising a product of likelihoods (maximise information) can be equivalently posed as minimising a sum of negative log-likelihoods or entropy (minimise uncertainty). The mutual information is then the information gained about \\(X\\) after learning \\(Y\\).\nA different interpretation comes from the first formulation which is concretely:\n\\[\n\\begin{align}\nI(X; Y) = \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} p(x, y) \\ln \\frac{p(x, y)}{p(x)p(y)}\n\\end{align}\n\\]\nTo interpret this I’ll briefly need to introduce the KL-Divergence which we can think of as a measure of distance between two distributions. We write it as \\(D_{KL}(p || q)\\) and it satisfies \\(D_{KL} \\geq 0\\) (Gibb’s inequality). The formula for the bivariate case is:\n\\[\n\\begin{align}\nD_{KL}(p(x, y) || q(x, y)) = \\sum_{x \\in \\mathcal{X}}\\sum_{y \\in \\mathcal{Y}} p(x, y) \\ln \\frac{p(x, y)}{q(x, y)}\n\\end{align}\n\\]\nWe generally view \\(p\\) as the ground truth distribution and \\(q\\) as some model distribution. The mutual information is then just \\(D_{KL}(p(x, y) || p(x)p(y))\\) and represents the deviation from independence displayed in the joint distribution. The property that \\(D_{KL} \\geq 0\\) ensures that this mutual information cannot be negative. In other words, conditioning on something cannot decrease the information you know about your random variable of interest.\n\n\nApplication to Machine Learning\nThe big entropy related measure I’ve ommitted entirely here is the Cross-Entropy. I think to properly motivate the Cross-Entropy a study on logistic regression or source coding helps - another time. That said, the mutual information and KL-divergence come up all the time - unsurprisingly as most ML tasks boil down to maximising a sum of log-likelihoods.\nTake a supervised learning scenario with dataset \\(\\mathcal{D} = \\{x_i, y_i\\}\\). Our model will implicitly learn a distribution \\(q(y | x, \\theta)\\) (where \\(\\theta\\) is the set of model parameters). Letting the true underlying distribution be \\(p(y | x)\\) we have \\(D_{KL}(p || q) \\geq 0\\) thus \\(\\mathbb{E}_p[\\ln p] \\geq \\mathbb{E}_p[\\ln q]\\). Jumbling some algebra together we get:\n\\[\n\\begin{align}\nI(X; Y) &= H[Y] - H[Y | X] \\\\\n&= H[Y] + \\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x, y) \\ln p(x | y) \\\\\n&= H[Y] + \\mathbb{E}_{p(x,y)}[ \\ln p(y | x)] \\\\\n&\\geq H[Y] + \\mathbb{E}_{p(x,y)}[ \\ln q(y | x, \\theta)] \\\\\n\\end{align}\n\\]\nWe should hope that a good model has objective of maximising the mutual information between features and labels and we can see that this optimisation is bounded by the true value.\n\n\nApplication to Statistics\nIf you do a lot of hypothesis testing you will be familiar with the Neyman-Pearson likelihood ratio:\n\\[\n\\begin{align}\n\\Lambda = \\frac{\\sup_{\\theta \\in \\Theta_1} p_{\\theta}(x)}{\\sup_{\\theta \\in \\Theta_0} p_{\\theta}(x)}\n\\end{align}\n\\]\n(Note I am now using the frequentist notation for \\(\\theta\\) where it is not a random variable)\nLikelihood and ratio already point to something like a KL-divergence. The rest is really just the same old trick. Take the case of hypothesis testing for dependence. Formally\n\\[\n\\begin{align}\n\\mathcal{H_0}: p_{\\theta}(x, y) = f(x)g(y) \\\\\n\\mathcal{H_1}: p_{\\theta}(x, y) \\neq f(x)g(y) \\\\\n\\end{align}\n\\]\nIf we take \\(N\\) iid samples from our distribution, we should look at the product of likelihood ratios…or its logarithm.\n\\[\n\\begin{align}\n\\ln \\Lambda &= \\sum_{i=1}^{N} \\ln p_{\\theta_j}(x_i, y_i) - \\ln p_{\\theta_f}(x_i, y_i) \\\\\n&= N \\mathbb{E}_{p(x, y)}[ \\ln p_{\\theta_j}(x_i, y_i)] - N \\mathbb{E}_{p(x, y)} [\\ln p_{\\theta_f}(x_i, y_i) ] \\\\\n&\\text{(in the weak law of large numbers limit)}\n\\end{align}\n\\]\n(where I am writing the null hypothesis factorised distribution as \\(p_{\\theta_f}(x_i, y_i)\\), the alternative hypothesis joint distribution as \\(p_{\\theta_j}(x_i, y_i)\\) and the true underlying distribution as \\(p(x, y)\\))\nLooking at each term of the expression in turn we have for the first term:\n\\[\n\\begin{align}\n\\mathbb{E}_{p(x, y)}[ \\ln p_{\\theta_j}(x_i, y_i)] &= \\mathbb{E}_{p(x, y)}[ \\ln p_{\\theta_j}(x_i, y_i) \\frac{p(x, y)}{p(x, y)}] \\\\\n&= -D_{KL}(p || p_{\\theta_j}) - H[X, Y] \\\\\n&= -H[X, Y] \\\\\n\\end{align}\n\\]\n(in the final line we take the extremal case where there does exist an optimal \\(\\theta_j\\) which makes the joint distribution from the alternative hypothesis exactly match the true underlying)\nFor the second term:\n\\[\n\\begin{align}\n\\mathbb{E}_{p(x, y)} [\\ln p_{\\theta_f}(x_i, y_i)] &= -D_{KL}(p(x) || p_{\\theta_f}(x)) - D_{KL}(p(y) || p_{\\theta_f}(y)) - H[X] - H[Y] \\\\\n&= -H[X] - H[Y]\n\\end{align}\n\\]\n(using a similar argument here for the extremal cases where the KL-divergence goes to zero)\nPutting it together we get \\(\\ln \\Lambda = N(H[X] + H[Y] - H[X, Y]) = N I(X; Y)\\). This makes a lot of sense retrospectively. If our variables are in fact independent, we expect \\(I = 0\\) which corresponds to a likelihood ratio of \\(1\\) - insufficient evidence to reject the null. If our variables are dependent, we expect a large mutual information and likelihood ratio - reject the null. We also see that the log ratio is linear in \\(N\\) thus more samples will always increase the likelihood ratio.\n\n\nThe End\nOne thing I want to make clear is that what I’ve presented is very much the configurational interpretation of entropy. The strange thing about entropy in physics, is that it appears in conjunction with energies and other thermodynamic quantities. This can of course be derived as well, I’m just not going to do it here. David Tong’s lecture notes do this very well - here."
  },
  {
    "objectID": "posts/variances_on_sphere.html",
    "href": "posts/variances_on_sphere.html",
    "title": "Variances on a sphere",
    "section": "",
    "text": "“Entities are not to be multiplied unnecessarily” John Punch\n\nIntroduction\nThe following discussion is very much against the above citation of Ockham’s Razor but is nonetheless entertaining. The problem is a simple one and you probably know the solution too (you might even know multiple solutions - if that’s the case, this post is likely a waste of your time). What is the variance of any of the coordinate \\(x, y, z\\) on a unit sphere?\n\n\nGeometric Solution\nBefore doing anything fancy, it should be obvious without calculation that the expected value of any of the three cartesian coordinates is zero as the probability mass in the opposing hemispheres is equal. If you’re a physicist, the following is probably the solution you’ve seen before. We’ll focus on the \\(z\\) coordinate - by symmetry they all have the same variance. We could jump straight to a trig integral and get to the answer in a few lines, but that implies assuming quite a bit of knowledge. The approach is broadly going to be:\n\ntransform to spherical coordinates with \\(r=1\\)\ndetermine the joint pdf of being at some location \\((\\theta, \\phi)\\) on the sphere\nextract the marginal pdf of being at some polar angle \\(\\theta\\)\nevaluate the variance in \\(z\\) knowing that \\(z = \\cos \\theta\\)\n\nThe coordinate transform to spherical coordinates is:\n\\[\n\\begin{align}\nx &= r\\sin\\theta \\cos\\phi \\\\\ny &= r\\sin\\theta \\sin\\phi \\\\\nz &= r\\cos\\theta\n\\end{align}\n\\]\nTo get the joint pdf we can either do a bit of sketching, or work with Jacobians. I’ll cover both:\n\n\n\nAn attempt at a sketch of a sphere\n\n\nSo we have that our area element is \\(dS = \\sin\\theta d\\theta d\\phi\\). In much the same way that we would calculate the probability of landing at some radius \\(r\\) in a circle of radius \\(R\\) by evaluating the ratio of areas \\(\\frac{\\pi r^2}{\\pi R^2}\\), we will do the same here - our area now being \\(4\\pi\\):\n\\[\n\\begin{align}\n\\mathbb{P}[\\theta &lt; \\theta' &lt; \\theta + d\\theta, \\phi &lt; &\\phi' &lt; \\phi + d\\phi] = \\frac{1}{4\\pi}\\sin\\theta \\, d\\theta \\, d\\phi \\\\\np(\\theta, \\phi) &= \\frac{1}{4\\pi}\\sin\\theta \\\\\np(\\theta) = \\int_{\\phi=0}^{\\phi=2\\pi} &p(\\theta, \\phi) d\\phi = \\frac{1}{2} \\sin\\theta\n\\end{align}\n\\]\nWith the marginal, the solution is easy. Let’s revisit the Jacobian. For a coordinate transform from coordinates \\(x_i\\) to \\(x_j\\) we have that \\(J_{ij} = \\frac{\\partial x_i}{\\partial x_j}\\). The Jacobian is really the transformation operator between the two systems and thus its determinant will tell us how much the generalised volume changes between the two. If in coordinates \\(x_i\\) we have a volume \\(dx_i^{(1)} dx_i^{(2)} dx_i^{(3)}\\), in \\(x_j\\) we will have a volume \\(\\det(J) dx_j^{(1)} dx_j^{(2)} dx_j^{(3)}\\). For our purposes \\(x_i = (x, y, z)\\) and \\(x_j = (r, \\theta, \\phi)\\). I’ll spare you the dirty work, the determinant of our Jacobian turns out to be \\(r^2 \\sin\\theta\\) thus:\n\\[\n\\begin{align}\n\\iiint f(x, y, z)\\, dx\\, dy\\, dz = \\iiint f(r\\sin\\theta\\cos\\phi, r\\sin\\theta\\sin\\phi, r\\cos\\theta) r^2 \\sin\\theta \\, dr \\, d\\theta \\, d\\phi\n\\end{align}\n\\]\nOur radius is constant so we can drop that dimension and look at the double integral instead. Let’s substitute \\(f(x, y, z)\\) as the pdf on the surface of the sphere which is uniform with probability \\(\\frac{1}{4\\pi}\\):\n\\[\n\\begin{align}\n\\iint \\frac{1}{4\\pi} \\sin\\theta \\, d\\theta \\, d\\phi = 1 \\text{ (by normalisation of a pdf)}\n\\end{align}\n\\]\nWe are unsurprisingly back at the same result for the joint pdf \\(p(\\theta, \\phi)\\). To conclude the solution we perform our variance calculation:\n\\[\n\\begin{align}\n\\mathbb{V}[z] = \\mathbb{V}[\\cos\\theta] = \\int_{0}^{\\pi} p(\\theta) \\cos^2\\theta \\, d\\theta - \\Bigg[ \\int_{0}^{\\pi} p(\\theta) \\cos\\theta \\, d\\theta \\Bigg]^2 = \\frac{1}{3}\n\\end{align}\n\\]\n\n\nCDF Solution\nSuppose you have the marginal \\(p(\\theta) = \\frac{1}{2} \\sin\\theta\\), we could also perform a variable transform to recover the marginal \\(p(z)\\). If we define a random variable \\(Y = g(X)\\) on some interval where \\(g\\) is a deterministic monotonic function then:\n\\[\n\\begin{align}\n\\mathbb{P}[Y &lt; y] &= \\int_{-\\infty}^{g^{-1}(y)} p_X(x) \\, dx = \\mathbb{P}[X &lt; g^{-1}(y)] \\text{ for increasing $g$} \\\\\n&= \\int_{g^{-1}(y)}^{\\infty} p_X(x) \\, dx = \\mathbb{P}[X &gt; g^{-1}(y)] \\text{ for decreasing $g$}\n\\end{align}\n\\]\nIn our case the interval of interest is \\(\\theta \\in [0, \\pi]\\) with \\(z = g(\\theta) = \\cos\\theta\\) where \\(\\cos\\) is monotonically decreasing over this interval. So we have that \\(\\mathbb{P}[Z &lt; z] = \\frac{1}{2}(1 + z)\\) from which we get that \\(p_Z(z) = \\frac{1}{2}\\). To some, this uniform marginal might be intuitive. On quick inspection, it seems that closer to the poles, there infinitesimal rings of constant \\(z\\) have smaller area than near the equator. It turns out that the rate at which these infinitesimal areas shrink near the poles is balanced by the rate at which the \\(z\\) coordinate changes with \\(\\theta\\). Anyhow, from here we again get that \\(\\mathbb{V}[Z] = \\frac{1}{3}\\).\n\n\nSymmetry Solution\nThis is by far the most economical solution and relies only on linearity of expectations. Our constraint is that \\(x^2 + y^2 + z^2 = 1\\) thus:\n\\[\n\\begin{align}\n&\\mathbb{E}[X^2 + Y^2 + Z^2 \\mid X^2 + Y^2 + Z^2 = 1] = 1 \\\\\n3&\\mathbb{E}[Z^2 \\mid X^2 + Y^2 + Z^2 = 1] = 1 \\rightarrow  \\mathbb{V}[Z] = \\frac{1}{3} \\\\\n\\end{align}\n\\]"
  },
  {
    "objectID": "posts/probabilistic_view_on_lin_reg.html",
    "href": "posts/probabilistic_view_on_lin_reg.html",
    "title": "A Probabilistic View on Linear Regression",
    "section": "",
    "text": "“Essentially, all models are wrong, but some are useful.” George E. P. Box\n\nIntroduction\nThis is by no means a new topic nor a particularly complex one, but I generally like to know where stuff in math or science really comes from. If you’re reading this, you are probably familiar with Linear Regression or Least Squares Regression (or whatever other name you’ve heard it called). The premise is that if we have some data points \\(y_i\\) and we measure a series of features \\(\\vec{x}^{(i)}\\), if our features are relevant predictors of our data, there might exist a linear (or affine) model which relates them:\n\\[\n\\begin{align}\ny_i = \\theta_1 x^{(i)}_1 + \\theta_2 x^{(i)}_2 + ... = \\vec{\\theta} \\cdot \\vec{x}^{(i)}\n\\end{align}\n\\]\nWe should think of \\(y_i\\) as the output, and each entry (\\(n\\) in total) of \\(\\vec{x}^{(i)}\\) as some feature. As good statisticians, we might measure \\(m\\) such outputs \\(y_i\\) and for each, their corresponding input feature vector \\(\\vec{x}^{(i)}\\). It is convenient to concatenate our \\(y_i\\) into a column vector \\(\\vec{y} \\in \\mathbb{R}^m\\) and our \\(\\vec{x}^{(i)}\\) vectors as rows of a matrix \\(X \\in \\mathbb{R}^{m \\times n}\\). For every pair \\((y_i, \\vec{x}^{(i)})\\) we will still use the same summation coefficients \\(\\vec{\\theta} \\in \\mathbb{R}^n\\).\n\\[\n\\begin{align}\n\\vec{y} = X \\vec{\\theta}\n\\end{align}\n\\]\nI personally prefer working in index notation (the arguments scale easily to arbitrary dimensions) so I’ll stick to that. So far we have been dealing with this idealised scenario where there exists this optimal projection direction which we’ll denote as \\(\\theta_j^{\\ast}\\). If this value exists, our optimisation task is simply to converge our model parameter \\(\\theta_j\\) to this value such that our model outputs \\(\\hat{y}_i\\) closely match the true measured values \\(y_i\\). I want to validate this claim of there existing such an optimal parameter \\(\\theta_j^{\\ast}\\).\n\n\n\nWhiteboard musings from this morning\n\n\n\n\nWhy Least Squares?\nWe can now start talking in terms of probability. Our measured data will almost surely not lie on a straight line, it’s noisy. We can think of each datum \\(y_i\\) as being drawn from some distribution centred on mean value \\(\\mu_i = X_{ij}\\theta_j^{\\ast}\\) with noise \\(\\xi_i\\). We take the noise to be gaussian with \\(\\mathbb{E}[\\xi_i] = 0\\) and \\(\\mathbb{V}[\\xi_i] = \\sigma^2\\delta_{ij}\\). Formally we have:\n\\[\n\\begin{align}\nY_i \\sim \\mathcal{N}(\\mu_i, \\sigma^2 \\delta_{ij})\n\\end{align}\n\\]\nFor each datum \\(y_i\\) there is a corresponding distribution. All have the same variance but the mean will be different - importantly, the mean is always coupled to the input feature vector. Equivalently, we can think of the concatenated vector of measured outputs \\(\\vec{y}\\) as being a sample from:\n\\[\n\\begin{align}\n\\vec{Y} \\sim \\mathcal{N}(\\vec{\\mu}, \\sigma^2 \\mathbb{I})\n\\end{align}\n\\]\nOur goal parameter \\(\\theta_j^{\\ast}\\) is inside the distribution’s mean. If we had many such vectors \\(\\vec{y}_i\\) all drawn from the same data matrix \\(X\\), the job of finding the mean would be straightforward (we’ll get back to this later). What if we only have one such \\(\\vec{y}\\)? Given an observed value of this random variable \\(Y_i\\), we can assign a probability to this sample given a mean \\(\\hat{\\mu}_i\\):\n\\[\n\\begin{align}\n\\hat{\\mu_i} &= X_{ij}\\theta_j \\\\\n\\mathbb{P}[Y_i = y_i \\mid \\vec{\\theta}, X] \\propto \\exp\\Bigg[-\\frac{1}{2} &(y_i - X_{ik}\\theta_k) (\\sigma^2 \\delta_{ij})^{-1} (y_j - X_{jk}\\theta_k) \\Bigg]\n\\end{align}\n\\]\nWe of course want the maximum likelihood estimator (MLE) of this mean. For a string of \\(m\\) such iid samples we can maximise the log-likelihood, omitting constant terms like the variance \\(\\sigma^2\\) or the normalisation factors, we have:\n\\[\n\\begin{align}\n\\vec{\\theta}^{\\ast} &= \\arg\\max_{\\theta} \\left\\{ \\sum_{i=1}^{m} -\\frac{1}{2} (y_i - X_{ik} \\theta_k)^2 \\right\\} \\\\\n&= \\arg\\min_{\\theta} \\left\\{ \\sum_{i=1}^{m} (y_i - X_{ik} \\theta_k)^2 \\right\\}\n\\end{align}\n\\]\nThis is now the familiar form of the problem with solution \\(\\vec{\\theta} = (X^T X)^{-1}X^T \\vec{y}\\). Now we can revisit the suggestion of arriving at this answer by collecting lots of sample vectors \\(\\vec{y}_i\\) (all from the same data matrix \\(X\\)) and finding their arithemtic mean. The process would like like this:\n\\[\n\\begin{align}\n\\langle \\vec{y} \\rangle &= \\frac{1}{N} \\sum_{i=1}^{N} \\vec{y}_i \\\\\n&= X\\vec{\\theta} \\\\\n\\rightarrow \\vec{\\theta} &= (X^T X)^{-1}X^T \\langle \\vec{y} \\rangle\n\\end{align}\n\\]\nWhere we are implicitly using the fact that \\(\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{i=1}^{N} \\vec{y}_i = \\mathbb{E}[\\vec{y}]\\) and the pseudo-inverse in the final line. Interestingly/unsurprisingly, we arrive at the same result.\n\n\nIncorporating Priors\nSo far we have made no assumption about the distribution of our summation coefficients \\(\\theta_j\\). Suppose we are trying to predict height in a sample of people. We might collect feature data for each person including: parent height, person weight, parent weight, …, annual salary etc. Intuitively, some feature variables will be more important and require larger summation coefficient or weighting than others. More generally, we want to make use of any prior knowledge of the distribution of \\(\\vec{\\theta}\\). This is known as regularisation. To do this, we should now maximise the log-likelihood of the joint probability \\(\\mathbb{P}[y_i, \\vec{\\theta} \\mid X] = \\mathbb{P}[y_i \\mid \\vec{\\theta}, X]\\mathbb{P}[\\vec{\\theta} \\mid X]\\) (in this discussion I am ignoring any probabilistic features in \\(X\\) thus \\(\\mathbb{P}[\\vec{\\theta} \\mid X] \\equiv \\mathbb{P}[\\theta]\\), the conditioning is a bit of a notational overload)\nA common assumption is that the weights are also gaussian with \\(\\theta_j \\sim \\mathcal{N}(0, \\tau^2)\\). Evaluating the product over \\(m\\) iid samples gives a joint likelihood of:\n\\[\n\\begin{align}\n\\mathbb{P}[\\vec{\\theta}] &\\propto \\prod_{j=1}^{n} \\exp\\Bigg[\\frac{\\theta_j^2}{2\\tau^2} \\Bigg] \\\\\n\\mathbb{P}[\\vec{y}, \\vec{\\theta} \\mid X] &\\propto \\prod_{i=1}^{m} \\exp\\Bigg[-\\frac{1}{2\\sigma^2} (y_i - X_{ik}\\theta_k)^2 \\Bigg] \\prod_{j=1}^{n} \\exp\\Bigg[-\\frac{\\theta_j^2}{2\\tau^2} \\Bigg] \\\\\n\\vec{\\theta}^{\\ast} &= \\arg\\min_{\\theta} \\left\\{ \\sum_{i=1}^{m} \\frac{1}{\\sigma^2} (y_i - X_{ik} \\theta_k)^2 + \\sum_{j=1}^{n} \\frac{\\theta_j^2}{\\tau^2} \\right\\} \\\\\n&= \\arg\\min_{\\theta} \\left\\{ \\sum_{i=1}^{m} (y_i - X_{ik} \\theta_k)^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2 \\right\\} \\\\\n\\text{where } \\lambda &= \\frac{\\sigma^2}{\\tau^2}\n\\end{align}\n\\]\nThis is the Ridge regression which we interpret as punishing large coefficients (in a loss function sense). The analytic solution is now \\(\\vec{\\theta} = (X^T X + \\lambda \\mathbb{I})^{-1}X^T \\vec{y}\\). A full bias-variance analysis (a topic for another post) reveals that this estimator is (unsurprisingly) biased but with lower variance. By restricting our search for parameters which also satisfy our assumption of the underlying parameter distribution, the spread of our optimal parameter will intuitively be smaller at the cost of bias.\nAnother common assumption of the underlying distribution is the Laplace or double-exponential assumption. This corresponds to \\(\\mathbb{P}[\\theta_j] = \\frac{1}{2b}\\exp [-\\frac{|\\theta_j|}{b}]\\). This symmetric distribution is much narrower than a gaussian with a much sharper gradient around zero. This shape is useful if we assume that not only are weights evenly distributed about zero, but many weights are exactly zero. Such an assumption is present in cases where we know that only a small subset of our feature space is relevant at all (but we don’t know apriori which features these are so we measure all of them anyway). An identical treatment to the above discussion on Ridge gives:\n\\[\n\\begin{align}\n\\vec{\\theta}^{\\ast} &= \\arg\\min_{\\theta} \\left\\{ \\sum_{i=1}^{m}(y_i - X_{ik}\\theta_k)^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j| \\right\\} \\\\\n\\text{where } \\lambda &= \\frac{2\\sigma^2}{b}\n\\end{align}\n\\]\nThis is the Lasso regression which also punishes large coefficients but is less forgiving for very small ones (unlike Ridge). The absolute value function is not differentiable at zero so there is no analytic solution for \\(\\vec{\\theta}\\). We usually proceed by sub-gradient descent (see here)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Blog",
    "section": "",
    "text": "A Probabilistic View on Linear Regression\n\n\n\n\n\n\n\n\nNov 1, 2025\n\n\nMichele Cespa\n\n\n\n\n\n\n\n\n\n\n\n\nSome thoughts on Information Geometry\n\n\n\n\n\n\n\n\nDec 12, 2025\n\n\nMichele Cespa\n\n\n\n\n\n\n\n\n\n\n\n\nVariances on a sphere\n\n\n\n\n\n\n\n\nOct 15, 2025\n\n\nMichele Cespa\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is Entropy?\n\n\n\n\n\n\n\n\nNov 19, 2025\n\n\nMichele Cespa\n\n\n\n\n\nNo matching items"
  }
]